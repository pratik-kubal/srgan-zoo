{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, Add\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.applications import VGG19\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from helper import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import pydot\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook\n",
    "import scipy\n",
    "import imageio\n",
    "from tensorlayer.prepro import *\n",
    "import tensorlayer as tl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_shape = (224,224,3)\n",
    "img_hr = Input(shape=hr_shape)\n",
    "lr_shape = (56,56,3)\n",
    "img_lr = Input(shape=lr_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/srgan/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "batchSize = 16\n",
    "optimizer = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0, amsgrad=False)\n",
    "epochs = 2000\n",
    "ni = np.sqrt(batchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(img_lr,lr_shape):\n",
    "    def residual_block(layer_input, filters):\n",
    "        \"\"\"Residual block described in paper\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=3, strides=1,\n",
    "                   padding='same')(layer_input)\n",
    "        d = Activation('relu')(d)\n",
    "        d = BatchNormalization(momentum=0.8)(d)\n",
    "        d = Conv2D(filters, kernel_size=3, strides=1, padding='same')(d)\n",
    "        d = BatchNormalization(momentum=0.8)(d)\n",
    "        d = Add()([d, layer_input])\n",
    "        return d\n",
    "\n",
    "    def deconv2d(layer_input):\n",
    "        \"\"\"Layers used during upsampling\"\"\"\n",
    "        u = UpSampling2D(size=2)(layer_input)\n",
    "        u = Conv2D(256, kernel_size=3, strides=1, padding='same')(u)\n",
    "        u = Activation('relu')(u)\n",
    "        return u\n",
    "\n",
    "    # Low resolution image input\n",
    "    img_lr = Input(shape=lr_shape)\n",
    "\n",
    "    # Pre-residual block\n",
    "    c1 = Conv2D(64, kernel_size=9, strides=1, padding='same')(img_lr)\n",
    "    c1 = Activation('relu')(c1)\n",
    "\n",
    "    # Propogate through residual blocks\n",
    "    r = residual_block(c1, 64)\n",
    "    for _ in range(16 - 1):\n",
    "        r = residual_block(r, 64)\n",
    "\n",
    "    # Post-residual block\n",
    "    c2 = Conv2D(64, kernel_size=3, strides=1, padding='same')(r)\n",
    "    c2 = BatchNormalization(momentum=0.8)(c2)\n",
    "    c2 = Add()([c2, c1])\n",
    "\n",
    "    # Upsampling\n",
    "    u1 = deconv2d(c2)\n",
    "    u2 = deconv2d(u1)\n",
    "\n",
    "    # Generate high resolution output\n",
    "    gen_hr = Conv2D(3, kernel_size=9, strides=1,\n",
    "                    padding='same', activation='tanh')(u2)\n",
    "\n",
    "    return Model(img_lr, gen_hr)\n",
    "\n",
    "gen = build_generator(img_lr,lr_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(gen, show_layer_names=True, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(hr_shape):\n",
    "\n",
    "        def d_block(layer_input, filters, strides=1, bn=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=3, strides=strides,\n",
    "                       padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        # Input img\n",
    "        d0 = Input(shape=hr_shape)\n",
    "\n",
    "        d1 = d_block(d0, 64, bn=False)\n",
    "        d2 = d_block(d1, 64, strides=2)\n",
    "        d3 = d_block(d2, 64*2)\n",
    "        d4 = d_block(d3, 64*2, strides=2)\n",
    "        d5 = d_block(d4, 64*4)\n",
    "        d6 = d_block(d5, 64*4, strides=2)\n",
    "        d7 = d_block(d6, 64*8)\n",
    "        d8 = d_block(d7, 64*8, strides=2)\n",
    "\n",
    "        d9 = Dense(64*16)(d8)\n",
    "        d10 = LeakyReLU(alpha=0.2)(d9)\n",
    "        validity = Dense(1, activation='sigmoid')(d10)\n",
    "\n",
    "        return Model(d0, validity)\n",
    "\n",
    "disc = build_discriminator(hr_shape)\n",
    "disc.compile(loss='mse',\n",
    "             optimizer=optimizer,\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(gen, show_layer_names=True, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vgg(hr_shape):\n",
    "    \"\"\"\n",
    "    Builds a pre-trained VGG19 model that outputs image features extracted at the\n",
    "    third block of the model\n",
    "    \"\"\"\n",
    "    vgg = VGG19(weights=\"imagenet\")\n",
    "    # Set outputs to outputs of last conv. layer in block 3\n",
    "    # See architecture at: https://github.com/keras-team/keras/blob/master/keras/applications/vgg19.py\n",
    "    vgg.outputs = [vgg.layers[9].output]\n",
    "\n",
    "    img = Input(shape=hr_shape)\n",
    "\n",
    "    # Extract image features\n",
    "    img_features = vgg(img)\n",
    "\n",
    "    return Model(img, img_features)\n",
    "\n",
    "vgg = build_vgg(hr_shape)\n",
    "vgg.trainable = False\n",
    "vgg.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_hr = gen(img_lr)\n",
    "vgg_features = vgg(gen_hr)\n",
    "validity = disc(gen_hr)\n",
    "combined = Model([img_lr, img_hr], [validity, vgg_features])\n",
    "combined.compile(loss=['binary_crossentropy', 'mse'],\n",
    "                              loss_weights=[1e-3, 1],\n",
    "                              optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(combined, show_layer_names=True, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./mainDataset\"\n",
    "dataset_name = \"train\"\n",
    "steps = len(glob(path+'/%s/*' % (dataset_name)))//batchSize\n",
    "# for the VGG feature out true labels\n",
    "patch = int(224 / 2**4)\n",
    "disc_patch = (patch, patch, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] read 32 from ./mainDataset/train/\n",
      "[TL] read 64 from ./mainDataset/train/\n",
      "[TL] read 96 from ./mainDataset/train/\n",
      "[TL] read 128 from ./mainDataset/train/\n",
      "[TL] read 160 from ./mainDataset/train/\n",
      "[TL] read 192 from ./mainDataset/train/\n",
      "[TL] read 224 from ./mainDataset/train/\n",
      "[TL] read 256 from ./mainDataset/train/\n",
      "[TL] read 288 from ./mainDataset/train/\n",
      "[TL] read 320 from ./mainDataset/train/\n",
      "[TL] read 352 from ./mainDataset/train/\n",
      "[TL] read 384 from ./mainDataset/train/\n",
      "[TL] read 416 from ./mainDataset/train/\n",
      "[TL] read 448 from ./mainDataset/train/\n",
      "[TL] read 480 from ./mainDataset/train/\n",
      "[TL] read 512 from ./mainDataset/train/\n",
      "[TL] read 544 from ./mainDataset/train/\n",
      "[TL] read 576 from ./mainDataset/train/\n",
      "[TL] read 608 from ./mainDataset/train/\n",
      "[TL] read 640 from ./mainDataset/train/\n",
      "[TL] read 672 from ./mainDataset/train/\n",
      "[TL] read 704 from ./mainDataset/train/\n",
      "[TL] read 736 from ./mainDataset/train/\n",
      "[TL] read 768 from ./mainDataset/train/\n",
      "[TL] read 800 from ./mainDataset/train/\n",
      "[TL] read 832 from ./mainDataset/train/\n",
      "[TL] read 864 from ./mainDataset/train/\n",
      "[TL] read 896 from ./mainDataset/train/\n",
      "[TL] read 928 from ./mainDataset/train/\n",
      "[TL] read 960 from ./mainDataset/train/\n",
      "[TL] read 992 from ./mainDataset/train/\n",
      "[TL] read 1024 from ./mainDataset/train/\n",
      "[TL] read 1056 from ./mainDataset/train/\n",
      "[TL] read 1088 from ./mainDataset/train/\n",
      "[TL] read 1120 from ./mainDataset/train/\n",
      "[TL] read 1152 from ./mainDataset/train/\n",
      "[TL] read 1184 from ./mainDataset/train/\n",
      "[TL] read 1216 from ./mainDataset/train/\n",
      "[TL] read 1248 from ./mainDataset/train/\n",
      "[TL] read 1280 from ./mainDataset/train/\n",
      "[TL] read 1312 from ./mainDataset/train/\n",
      "[TL] read 1344 from ./mainDataset/train/\n",
      "[TL] read 1376 from ./mainDataset/train/\n",
      "[TL] read 1408 from ./mainDataset/train/\n",
      "[TL] read 1440 from ./mainDataset/train/\n",
      "[TL] read 1472 from ./mainDataset/train/\n",
      "[TL] read 1504 from ./mainDataset/train/\n",
      "[TL] read 1536 from ./mainDataset/train/\n",
      "[TL] read 1568 from ./mainDataset/train/\n",
      "[TL] read 1600 from ./mainDataset/train/\n",
      "[TL] read 1632 from ./mainDataset/train/\n",
      "[TL] read 1664 from ./mainDataset/train/\n",
      "[TL] read 1696 from ./mainDataset/train/\n",
      "[TL] read 1728 from ./mainDataset/train/\n",
      "[TL] read 1760 from ./mainDataset/train/\n",
      "[TL] read 1792 from ./mainDataset/train/\n",
      "[TL] read 1824 from ./mainDataset/train/\n",
      "[TL] read 1856 from ./mainDataset/train/\n",
      "[TL] read 1888 from ./mainDataset/train/\n",
      "[TL] read 1920 from ./mainDataset/train/\n",
      "[TL] read 1952 from ./mainDataset/train/\n",
      "[TL] read 1984 from ./mainDataset/train/\n",
      "[TL] read 2016 from ./mainDataset/train/\n",
      "[TL] read 2048 from ./mainDataset/train/\n",
      "[TL] read 2080 from ./mainDataset/train/\n",
      "[TL] read 2112 from ./mainDataset/train/\n",
      "[TL] read 2144 from ./mainDataset/train/\n",
      "[TL] read 2176 from ./mainDataset/train/\n",
      "[TL] read 2208 from ./mainDataset/train/\n",
      "[TL] read 2240 from ./mainDataset/train/\n",
      "[TL] read 2272 from ./mainDataset/train/\n",
      "[TL] read 2304 from ./mainDataset/train/\n",
      "[TL] read 2336 from ./mainDataset/train/\n",
      "[TL] read 2368 from ./mainDataset/train/\n",
      "[TL] read 2400 from ./mainDataset/train/\n",
      "[TL] read 2432 from ./mainDataset/train/\n",
      "[TL] read 2464 from ./mainDataset/train/\n",
      "[TL] read 2496 from ./mainDataset/train/\n",
      "[TL] read 2528 from ./mainDataset/train/\n",
      "[TL] read 2560 from ./mainDataset/train/\n",
      "[TL] read 2592 from ./mainDataset/train/\n",
      "[TL] read 2624 from ./mainDataset/train/\n",
      "[TL] read 2656 from ./mainDataset/train/\n",
      "[TL] read 2688 from ./mainDataset/train/\n",
      "[TL] read 2720 from ./mainDataset/train/\n",
      "[TL] read 2752 from ./mainDataset/train/\n",
      "[TL] read 2784 from ./mainDataset/train/\n",
      "[TL] read 2816 from ./mainDataset/train/\n",
      "[TL] read 2848 from ./mainDataset/train/\n",
      "[TL] read 2880 from ./mainDataset/train/\n",
      "[TL] read 2912 from ./mainDataset/train/\n",
      "[TL] read 2920 from ./mainDataset/train/\n"
     ]
    }
   ],
   "source": [
    "train_hr_img_list = sorted(tl.files.load_file_list(path=path+'/%s/' % (dataset_name), regx='.*.png', printable=False))\n",
    "\n",
    "train_hr_imgs = tl.vis.read_images(train_hr_img_list, path=path+'/%s/' % (dataset_name), n_threads=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(\n",
    "  log_dir='log/pretrain_fix/run3',\n",
    "  histogram_freq=0,\n",
    "  batch_size=batchSize,\n",
    "  write_graph=True,\n",
    "  write_grads=True\n",
    ")\n",
    "tensorboard.set_model(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSavePath = 'checkpoints/'\n",
    "if not os.path.exists(modelSavePath):\n",
    "    os.makedirs(modelSavePath)\n",
    "\n",
    "modelcheckpoint = ModelCheckpoint(\n",
    "    filepath = modelSavePath+\"baseline.hdf5\",\n",
    "    monitor='g_loss',\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    save_best_only=True\n",
    ")\n",
    "modelcheckpoint.set_model(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen.load_weights(\"checkpoints/gen_init.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen(dev_hr_imgs,batchSize,is_testing=False):\n",
    "    while(True):\n",
    "        imgs_hr=[]\n",
    "        imgs_lr=[]\n",
    "        imgs = np.random.choice(dev_hr_imgs,batchSize)\n",
    "        img_hr = tl.prepro.threading_data(imgs, fn=crop, wrg=224, hrg=224, is_random=True)\n",
    "        img_lr = tl.prepro.threading_data(img_hr, fn=imresize,size=[56, 56], interp='bicubic', mode=None)\n",
    "    \n",
    "        imgs_hr = np.array(img_hr) / 127.5 - 1.\n",
    "        imgs_lr = np.array(img_lr) / 127.5 - 1.\n",
    "\n",
    "        yield imgs_hr, imgs_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagenObj = datagen(train_hr_imgs,batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_hr = tl.prepro.threading_data(train_hr_imgs[0:batchSize], fn=crop, wrg=224, hrg=224, is_random=True)\n",
    "sample_lr = tl.prepro.threading_data(sample_hr, fn=imresize,size=[56, 56], interp='bicubic', mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.vis.save_images(sample_hr, [int(ni), int(ni)],'images/'+dataset_name+'/sample_hr.png')\n",
    "tl.vis.save_images(sample_lr, [int(ni), int(ni)],'images/'+dataset_name+'/sample_lr.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7b86657aeb4491867c08577c38fe46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=182), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/srgan/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "Epoch 0 time: 0:00:01.643863\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b59526b9c5419e81bf77b580cde321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=182), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 time: 0:00:00.442170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94941ddfd632434c84012c6955a70d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=182), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 time: 0:00:00.440416\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9233043e8b4747b4ad7c77dc2d3e4cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=182), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 time: 0:00:00.440872\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059fa66b038b4c90943d906c17160b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=182), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 time: 0:00:00.442805\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d53cabfecd0474792701c2286e036ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=182), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 time: 0:00:00.442854\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9179ca55304c898ffc23bf9fba26da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=182), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 time: 0:00:00.437036\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a284281edf4734acac3d82fe0d58c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=182), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 time: 0:00:00.435544\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ee391b50894cdfbb5b6527d95edcae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=182), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 time: 0:00:00.440912\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ba356bf9204281966551f6b6c71873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=182), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 time: 0:00:00.441503\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de60a5dd3b6477681a8aa00d7d7b50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=182), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    for step in tqdm_notebook(range(0,steps)):\n",
    "        start_time = datetime.datetime.now()      \n",
    "        # ------------------\n",
    "        #  Train Generator\n",
    "        # ------------------\n",
    "\n",
    "        # Sample images and their conditioning counterparts\n",
    "        imgs_hr, imgs_lr = next(datagenObj)\n",
    "\n",
    "        # The generators want the discriminators to label the generated images as real\n",
    "        valid = np.random.uniform(low=0.6, high=1, size=((batchSize,) + disc_patch))\n",
    "\n",
    "        # Extract ground truth image features using pre-trained VGG19 model\n",
    "        image_features = vgg.predict(imgs_hr)\n",
    "\n",
    "        # Train the generators\n",
    "        g_loss = combined.train_on_batch([imgs_lr, imgs_hr], [valid, image_features])\n",
    "        \n",
    "        tb_step = step + int(epoch*(steps))\n",
    "        tensorboard.on_epoch_end(tb_step, {\"g_int_loss\":g_loss[0]})\n",
    "        \n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        out = gen.predict(sample_lr)\n",
    "        tl.vis.save_images(out, [int(ni), int(ni)],'images/'+dataset_name+'/A_train.png')\n",
    "\n",
    "    elapsed_time = datetime.datetime.now() - start_time\n",
    "    gen.save_weights(\"./checkpoints/gen_init.h5\")\n",
    "    # Plot the progress\n",
    "    print(\"Epoch %d time: %s\" % (epoch, elapsed_time))\n",
    "\n",
    "    # If at save interval => save generated image samples\n",
    "tensorboard.on_train_end(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(\n",
    "  log_dir='log/train_fix/run4',\n",
    "  histogram_freq=0,\n",
    "  batch_size=batchSize,\n",
    "  write_graph=True,\n",
    "  write_grads=True\n",
    ")\n",
    "tensorboard.set_model(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SRGAN - 2and 3\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_idx = 0\n",
    "g_idx = 0\n",
    "for epoch in range(epochs):\n",
    "    for step in tqdm_notebook(range(0,steps)):\n",
    "        start_time = datetime.datetime.now()     \n",
    "        if step % 2 == 0:\n",
    "            # ----------------------\n",
    "            #  Train Discriminator\n",
    "            # ----------------------\n",
    "            d_idx += 1\n",
    "            # Sample images and their conditioning counterparts\n",
    "            imgs_hr, imgs_lr = next(datagenObj)\n",
    "\n",
    "            # From low res. image generate high res. version\n",
    "            fake_hr = gen.predict(imgs_lr)\n",
    "\n",
    "            valid = np.random.uniform(low=0.8, high=1, size=((batchSize,) + disc_patch))\n",
    "            fake = np.random.uniform(low=0, high=0.2, size=((batchSize,) + disc_patch))\n",
    "\n",
    "            # Train the discriminators (original images = real / generated = Fake)\n",
    "            disc.trainable = True\n",
    "            d_loss_real = disc.train_on_batch(imgs_hr, valid)\n",
    "            d_loss_fake = disc.train_on_batch(fake_hr, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            disc.trainable = False\n",
    "            \n",
    "            tensorboard.on_epoch_end(d_idx, {\"d_loss\": d_loss[0]})\n",
    "            \n",
    "        else:\n",
    "            # ------------------\n",
    "            #  Train Generator\n",
    "            # ------------------\n",
    "            g_idx += 1\n",
    "            # Sample images and their conditioning counterparts\n",
    "            imgs_hr, imgs_lr = next(datagenObj)\n",
    "\n",
    "            # The generators want the discriminators to label the generated images as real\n",
    "            valid = np.random.uniform(low=0.9, high=1, size=((batchSize,) + disc_patch))\n",
    "\n",
    "            # Extract ground truth image features using pre-trained VGG19 model\n",
    "            image_features = vgg.predict(imgs_hr)\n",
    "\n",
    "            # Train the generators\n",
    "            g_loss = combined.train_on_batch([imgs_lr, imgs_hr], [valid, image_features])\n",
    "            tensorboard.on_epoch_end(g_idx, {\"g_loss\":g_loss[0]})\n",
    "        \n",
    "    out = gen.predict(sample_lr)\n",
    "    tl.vis.save_images(out, [int(ni), int(ni)],'images/'+dataset_name+'/train_%d.png' % int(epoch*steps + step))\n",
    "    if(epoch % 100 == 0):\n",
    "        out = gen.predict(sample_lr)\n",
    "        tl.vis.save_images(out, [int(ni), int(ni)],'images/'+dataset_name+'/train_%d.png' % int(epoch))\n",
    "    elapsed_time = datetime.datetime.now() - start_time\n",
    "    gen.save_weights(\"./checkpoints/gen.h5\")\n",
    "    disc.save_weights(\"./checkpoints/disc.h5\")\n",
    "    # Plot the progress\n",
    "    print(\"Epoch %d time: %s\" % (epoch, elapsed_time))\n",
    "tensorboard.on_train_end(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " elapsed_time = datetime.datetime.now() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
